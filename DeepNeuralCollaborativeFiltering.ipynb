{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepNeuralCollaborativeFiltering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W5VrFoRiHsP"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXoKj75oD_zw"
      },
      "source": [
        "Files obtained from UCSD Book Graph dataset from Goodreads:\n",
        "https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXMHgzuOCymA"
      },
      "source": [
        "Load the huge csv file containing comics and graphic novel interaction data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkNzWJdPCsR5"
      },
      "source": [
        "df_interactions = pd.read_json(\"./goodreads_interactions_comics_graphic.json.gz\",lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTxMmfj0C6WH"
      },
      "source": [
        "# Filter out entries with \"is_read\"==False, \n",
        "# users with less than 5 books read, \n",
        "# and any books with less than 75 reviews\n",
        "# select columns \"user_id\", \"book_id\", \"is_read\", \"rating\"\n",
        "df_interactions = df_interactions.loc[\n",
        "    df_interactions[\"user_id\"].map( df_interactions[\"user_id\"].value_counts() >= 5)&\\\n",
        "    df_interactions[\"book_id\"].map(df_interactions[\"book_id\"].value_counts()>=75)&\\\n",
        "    df_interactions[\"is_read\"],\\\n",
        "    [\"user_id\",\"book_id\",\"is_read\",\"rating\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2IynFRzELX6"
      },
      "source": [
        "Replace all \"book_id\" with title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sfkZu7_DuXP"
      },
      "source": [
        "# create hashtable from 'book_id' to 'title' \n",
        "book_table = {}\n",
        "with gzip.open(\"./goodreads_books_comics_graphic.json.gz\") as fin:\n",
        "    for l in fin:\n",
        "        d = json.loads(l)\n",
        "        book_table[d[\"book_id\"]] = d[\"title\"]\n",
        "\n",
        "# map each \"book_id\" to \"title\"\n",
        "df_interactions[\"book_title\"] = df_interactions[\"book_id\"].apply(str).map(book_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmI3GAJfl2-S"
      },
      "source": [
        "# label all books by numbers 1,...,len(book_list)\n",
        "book_list = np.array(list(set(df_interactions[\"book_title\"])))\n",
        "n_books = len(book_list)\n",
        "book_table = { book_list[i]: i for i in range(len(book_list))}\n",
        "df_interactions[\"book_num\"] = df_interactions[\"book_title\"].map(book_table)\n",
        "\n",
        "# label users by numbers 1,...,len(user_list)\n",
        "user_list = np.array(list(set(df_interactions[\"user_num\"])))\n",
        "user_table = { user_list[i] : i for i in range(len(user_list))}\n",
        "n_users = len(user_list)\n",
        "df_interactions[\"user_num\"] = df_interactions[\"user_num\"].map(user_table)\n",
        "\n",
        "# drop \"user_id\", \"is_read\", and \"book_id\"\n",
        "df_interactions = df_interactions[[\"user_num\",\"book_num\",\"book_title\",\"rating\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXSBIYYOjhXh"
      },
      "source": [
        "# Popularity Model\n",
        "\n",
        "Find the top 10 most read graphic novels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqqp2-Ovp7Po",
        "outputId": "a0295575-fbc5-43b9-a2c8-356a705adafd"
      },
      "source": [
        "df_interactions.book_title.value_counts().head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Watchmen                                                          23530\n",
              "Saga, Vol. 1 (Saga, #1)                                           15704\n",
              "V for Vendetta                                                    13165\n",
              "Preludes & Nocturnes (The Sandman #1)                             12767\n",
              "Saga, Vol. 2 (Saga, #2)                                           12003\n",
              "Maus I: A Survivor's Tale: My Father Bleeds History (Maus, #1)    10715\n",
              "Saga, Vol. 3 (Saga, #3)                                           10460\n",
              "The Walking Dead, Vol. 01: Days Gone Bye                          10173\n",
              "Batman: The Killing Joke                                           9939\n",
              "Persepolis: The Story of a Childhood (Persepolis, #1)              9227\n",
              "Name: book_title, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvS5h-QKHdsm"
      },
      "source": [
        "# Training and Testing Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CWPTivqZWw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-63Rh1TWv8"
      },
      "source": [
        "pairs = []\n",
        "for i, row in df_interactions[[\"user_num\",\"book_num\"]].sort_values(by=[\"user_num\",\"book_num\"],ascending=True).iterrows():\n",
        "  pairs.append((row[\"user_num\"],row[\"book_num\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxy4Mxq1kXfn"
      },
      "source": [
        "def binarySearch(pairs, el):\n",
        "  u, b = el\n",
        "  L=0\n",
        "  R=len(pairs)-1\n",
        "  mid = (L+R)//2\n",
        "  while L <= R:\n",
        "    mid = (L+R)//2\n",
        "    if pairs[mid][0] < u:\n",
        "      L = mid+1\n",
        "    elif pairs[mid][0] > u:\n",
        "      R = mid-1\n",
        "    else:\n",
        "      if pairs[mid][1] < b:\n",
        "        L = mid+1\n",
        "      elif pairs[mid][1] > b:\n",
        "        R = mid-1\n",
        "      else:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "# hold 2.5% of data aside as testing data\n",
        "pairs_train, pairs_test = train_test_split(pairs, test_size=0.025, random_state=99, shuffle=False)\n",
        "\n",
        "# also generate negative pairs to hold for testing data. Hold a ratio of 7:1.\n",
        "neg_ratio = 7\n",
        "neg_test = []\n",
        "i=0\n",
        "while i < neg_ratio*len(pairs_test):\n",
        "    rand_user = random.randrange(len(user_list))\n",
        "    rand_book = random.randrange(len(book_list))\n",
        "    # check that (rand_user, rand_book) is not in pairs set and not in validation set\n",
        "    if not binarySearch(pairs, (rand_user, rand_book)):\n",
        "        neg_test.append((rand_user,rand_book))\n",
        "        i+=1\n",
        "neg_test.sort()\n",
        "\n",
        "# use positive pairs and negative pairs to create validation set\n",
        "def val_set(pos, neg, classification=False):\n",
        "    if classification:\n",
        "        neg_val = 0\n",
        "    else:\n",
        "        neg_val = -1\n",
        "    batch = np.zeros( (len(pairs_test) + len(neg_test), 3) )\n",
        "    i=0\n",
        "    for user, book in pos:\n",
        "        batch[i,:] = (user, book, 1)\n",
        "        i+=1\n",
        "    for user, book in neg:\n",
        "        batch[i,:] = (user, book, neg_val)\n",
        "    return batch\n",
        "\n",
        "validation_set = val_set(pairs_test, neg_test)\n",
        "\n",
        "# for the remaining training pairs, define a generator to select positive and negative pairs\n",
        "# do not include pairs that occur in testing sets: pairs_test and neg_test\n",
        "def generate_pairs(pairs_train, n_positive=64, neg_ratio=1, pairs_test=[], neg_test=[], classification=False):\n",
        "    size = n_positive*(1 + neg_ratio)\n",
        "    batch = np.zeros((int(size), 3))\n",
        "\n",
        "    if classification:\n",
        "        neg_val = 0\n",
        "    else:\n",
        "        neg_val = -1\n",
        "    while True:\n",
        "        for i, (user, book) in enumerate(random.sample(pairs, n_positive)):\n",
        "            batch[i,:] = (user, book, 1)\n",
        "        i+=1\n",
        "        while i < size:\n",
        "            rand_user = random.randrange(len(user_list))\n",
        "            rand_book = random.randrange(len(book_list))\n",
        "            # check that (rand_user, rand_book) is not in pairs set and not in validation set\n",
        "            if not binarySearch(pairs_train, (rand_user, rand_book)) and \\\n",
        "                not binarySearch(pairs_test, (rand_user, rand_book)) and \\\n",
        "                not binarySearch( neg_test, (rand_user, rand_book) ):\n",
        "                batch[i,:] = (rand_user, rand_book, neg_val)\n",
        "                i+=1\n",
        "        np.random.shuffle(batch)\n",
        "        yield {\"user_input\": batch[:,0], \"book_input\": batch[:,1]}, batch[:,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APCPKtsGIIhV",
        "outputId": "6d530547-bc71-46a3-c8ca-f664c69c94b1"
      },
      "source": [
        "next(generate_pairs(pairs_train,2,2,pairs_test,neg_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'book_input': array([8142., 1593., 8191., 4864., 1105., 2437.]),\n",
              "  'user_input': array([49244., 86833., 26391., 43765., 47495., 44097.])},\n",
              " array([-1.,  1., -1., -1., -1.,  1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dcSVu_hQE1h"
      },
      "source": [
        "# Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdyWr3_HqhR8"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Dot, Flatten, Dense, Reshape, Concatenate\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7_uLX98iVnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161b1419-8d69-4a58-d9c2-91c48cc19c25"
      },
      "source": [
        "feature_dim = 64\n",
        "\n",
        "\n",
        "book_input = Input(shape = [1], name=\"book_input\")\n",
        "book_embedding = Embedding(n_books,feature_dim, name=\"book_embedding\")(book_input)\n",
        "book_vec = Flatten(name=\"book_vec\")(book_embedding)\n",
        "user_input = Input(shape = [1], name=\"user_input\")\n",
        "user_embedding = Embedding(n_users,feature_dim,name = \"user_embedding\")(user_input)\n",
        "user_vec = Flatten(name=\"user_vec\")(user_embedding)\n",
        "x = Dot(axes=1,name=\"DotProduct\")([user_vec,book_vec])\n",
        "\n",
        "MatrixFactorization = Model(inputs = [user_input,book_input], outputs=x, name=\"FeatureEmbeddings\")\n",
        "MatrixFactorization.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
        "\n",
        "MatrixFactorization.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"FeatureEmbeddings\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "user_input (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "book_input (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_embedding (Embedding)      (None, 1, 64)        6136768     user_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "book_embedding (Embedding)      (None, 1, 64)        664896      book_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "user_vec (Flatten)              (None, 64)           0           user_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "book_vec (Flatten)              (None, 64)           0           book_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "DotProduct (Dot)                (None, 1)            0           user_vec[0][0]                   \n",
            "                                                                 book_vec[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,801,664\n",
            "Trainable params: 6,801,664\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9RxzfybTfJ"
      },
      "source": [
        "Train the model using only train data, evaluate fit using test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4ZYvufqsfzF",
        "outputId": "1404bca6-1a73-4ccb-a721-6990feb4bc30"
      },
      "source": [
        "# Future runs use epochs=7\n",
        "n_positive = 2048\n",
        "neg_ratio = 7\n",
        "\n",
        "gen = generate_pairs(pairs_train, n_positive, neg_ratio, pairs_test, neg_test)\n",
        "\n",
        "hist = MatrixFactorization.fit(gen, epochs=7, steps_per_epoch = len(pairs_train)//n_positive, \n",
        "    validation_data = ( [validation_set[:,0], validation_set[:,1] ], validation_set[:,2] ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "1632/1632 [==============================] - 1028s 628ms/step - loss: 0.7235 - val_loss: 0.7378\n",
            "Epoch 2/7\n",
            "1632/1632 [==============================] - 1019s 625ms/step - loss: 0.2039 - val_loss: 0.6345\n",
            "Epoch 3/7\n",
            "1632/1632 [==============================] - 1009s 618ms/step - loss: 0.1598 - val_loss: 0.5705\n",
            "Epoch 4/7\n",
            "1632/1632 [==============================] - 1009s 619ms/step - loss: 0.1356 - val_loss: 0.5114\n",
            "Epoch 5/7\n",
            "1632/1632 [==============================] - 1009s 618ms/step - loss: 0.1222 - val_loss: 0.5167\n",
            "Epoch 6/7\n",
            "1632/1632 [==============================] - 1003s 615ms/step - loss: 0.1145 - val_loss: 0.4642\n",
            "Epoch 7/7\n",
            "1632/1632 [==============================] - 1009s 618ms/step - loss: 0.1097 - val_loss: 0.3659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2NqtmReKx9L",
        "outputId": "0c054632-5690-4390-92e3-2599b08c2f30"
      },
      "source": [
        "MatrixFactorization.save(\"/content/drive/MyDrive/models/MatrixFactorization\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/MatrixFactorization/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1kjOTSqYWVT"
      },
      "source": [
        "**Get weights and find similar books**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoV1yCZ0Tdzs"
      },
      "source": [
        "book_layer = MatrixFactorization.get_layer(\"book_embedding\")\n",
        "book_weights = book_layer.get_weights()[0]\n",
        "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))\n",
        "\n",
        "# Find books similar to \"book_name\"\n",
        "def similarBooks(book_name):\n",
        "  book_num = book_table[book_name]\n",
        "\n",
        "  similarity = np.dot(book_weights, book_weights[book_num])\n",
        "\n",
        "  sorted_similarity = np.argsort(similarity)\n",
        "  closest = sorted_similarity[-11:-1]\n",
        "\n",
        "  maxlength = max([len(book_list[c]) for c in closest])\n",
        "\n",
        "  print(f\"Books closest to {book_name}:\")\n",
        "  for c in reversed(closest):\n",
        "    print(f\"Book: {book_list[c]:{maxlength+2}}Sim: {similarity[c]:.{3}}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2UnRozZThCJ",
        "outputId": "3653f77d-c3e8-444a-f1e4-cef3287e8ed9"
      },
      "source": [
        "similarBooks(\"The Hard Goodbye (Sin City #1)\")\n",
        "print(\"\\n\")\n",
        "similarBooks(\"Watchmen\")\n",
        "print(\"\\n\")\n",
        "similarBooks(\"Batman: The Dark Knight Returns (The Dark Knight Saga, #1)\")\n",
        "print(\"\\n\")\n",
        "similarBooks(\"Captain America: Winter Soldier, Volume 1\")\n",
        "print(\"\\n\")\n",
        "similarBooks(\"300\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Books closest to The Hard Goodbye (Sin City #1):\n",
            "Book: A Dame to Kill For (Sin City #2)                            Sim: 0.869\n",
            "Book: That Yellow Bastard (Sin City #4)                           Sim: 0.818\n",
            "Book: 300                                                         Sim: 0.784\n",
            "Book: The Big Fat Kill (Sin City #3)                              Sim: 0.784\n",
            "Book: Family Values (Sin City #5)                                 Sim: 0.712\n",
            "Book: Booze, Broads, and Bullets (Sin City #6)                    Sim: 0.683\n",
            "Book: Hell and Back (Sin City #7)                                 Sim: 0.653\n",
            "Book: From Hell                                                   Sim: 0.646\n",
            "Book: V for Vendetta                                              Sim: 0.613\n",
            "Book: Batman: The Dark Knight Returns (The Dark Knight Saga, #1)  Sim: 0.603\n",
            "\n",
            "\n",
            "Books closest to Watchmen:\n",
            "Book: V for Vendetta                                                  Sim: 0.812\n",
            "Book: Preludes & Nocturnes (The Sandman #1)                           Sim: 0.587\n",
            "Book: Batman: The Killing Joke                                        Sim: 0.568\n",
            "Book: Batman: The Dark Knight Returns (The Dark Knight Saga, #1)      Sim: 0.549\n",
            "Book: Maus I: A Survivor's Tale: My Father Bleeds History (Maus, #1)  Sim: 0.531\n",
            "Book: The Complete Maus (Maus, #1-2)                                  Sim: 0.505\n",
            "Book: Batman: Year One                                                Sim: 0.503\n",
            "Book: The League of Extraordinary Gentlemen, Vol. 1                   Sim: 0.488\n",
            "Book: The Complete Persepolis                                         Sim: 0.442\n",
            "Book: Y: The Last Man, Vol. 1: Unmanned                               Sim: 0.439\n",
            "\n",
            "\n",
            "Books closest to Batman: The Dark Knight Returns (The Dark Knight Saga, #1):\n",
            "Book: Batman: Year One                                                  Sim: 0.95\n",
            "Book: Batman: The Killing Joke                                          Sim: 0.895\n",
            "Book: Batman: The Long Halloween                                        Sim: 0.864\n",
            "Book: Batman: Arkham Asylum - A Serious House on Serious Earth          Sim: 0.811\n",
            "Book: Superman: Red Son                                                 Sim: 0.752\n",
            "Book: Kingdom Come                                                      Sim: 0.735\n",
            "Book: Batman: The Dark Knight Strikes Again (The Dark Knight Saga, #2)  Sim: 0.687\n",
            "Book: Batman: Dark Victory                                              Sim: 0.675\n",
            "Book: The Joker                                                         Sim: 0.655\n",
            "Book: V for Vendetta                                                    Sim: 0.645\n",
            "\n",
            "\n",
            "Books closest to Captain America: Winter Soldier, Volume 1:\n",
            "Book: Captain America: Winter Soldier, Volume 2                                            Sim: 0.946\n",
            "Book: Captain America: The Death of Captain America, Volume 1: The Death of the Dream      Sim: 0.887\n",
            "Book: Captain America: The Death of Captain America, Volume 2: The Burden of Dreams        Sim: 0.874\n",
            "Book: Captain America: Reborn                                                              Sim: 0.841\n",
            "Book: Captain America: The Death Of Captain America, Volume 3: The Man Who Bought America  Sim: 0.837\n",
            "Book: The Invincible Iron Man, Volume 1: The Five Nightmares                               Sim: 0.833\n",
            "Book: The New Avengers, Volume 1: Breakout                                                 Sim: 0.827\n",
            "Book: Captain America: Red Menace, Volume 2                                                Sim: 0.826\n",
            "Book: Captain America: Red Menace, Volume 1                                                Sim: 0.813\n",
            "Book: Captain America: Red Menace                                                          Sim: 0.803\n",
            "\n",
            "\n",
            "Books closest to 300:\n",
            "Book: The Hard Goodbye (Sin City #1)                                    Sim: 0.784\n",
            "Book: That Yellow Bastard (Sin City #4)                                 Sim: 0.669\n",
            "Book: From Hell                                                         Sim: 0.667\n",
            "Book: A Dame to Kill For (Sin City #2)                                  Sim: 0.661\n",
            "Book: Batman: Year One                                                  Sim: 0.65\n",
            "Book: Batman: The Dark Knight Returns (The Dark Knight Saga, #1)        Sim: 0.638\n",
            "Book: Batman: The Dark Knight Strikes Again (The Dark Knight Saga, #2)  Sim: 0.622\n",
            "Book: The Big Fat Kill (Sin City #3)                                    Sim: 0.621\n",
            "Book: Kick-Ass (Kick-Ass, #1)                                           Sim: 0.619\n",
            "Book: 30 Days of Night, Vol. 1                                          Sim: 0.604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiQEg8soIxrn"
      },
      "source": [
        "# Deep Neural Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8s-BmaMql_N"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Gwe8gguyCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4207acab-e4e9-413d-ddd2-9e6d4f3b851f"
      },
      "source": [
        "feature_dim = 16\n",
        "\n",
        "\n",
        "book_input = Input(shape = [1], name=\"book_input\")\n",
        "book_embedding = Embedding(n_books,feature_dim, name=\"book_embedding\")(book_input)\n",
        "book_vec = Flatten(name=\"book_vec\")(book_embedding)\n",
        "user_input = Input(shape = [1], name=\"user_input\")\n",
        "user_embedding = Embedding(n_users,feature_dim,name = \"user_embedding\")(user_input)\n",
        "user_vec = Flatten(name=\"user_vec\")(user_embedding)\n",
        "x = Concatenate(axis=1,name=\"Cat\")([user_vec,book_vec])\n",
        "x = Dense(16,activation=\"relu\", name=\"hidden_layer_1\")(x)\n",
        "x = Dense(16,activation=\"relu\", name=\"hidden_layer_2\")(x)\n",
        "x = Dense(32,activation=\"relu\", name=\"hidden_layer_3\")(x)\n",
        "x = Dense(32,activation=\"relu\", name=\"hidden_layer_4\")(x)\n",
        "x = Dense(16,activation=\"relu\", name=\"hidden_layer_5\")(x)\n",
        "x = Dense(16,activation=\"relu\", name=\"hidden_layer_6\")(x)\n",
        "x = Dense(1, activation=\"sigmoid\", name=\"sigmoid_out\")(x)\n",
        "\n",
        "dncf = Model(inputs = [user_input,book_input], outputs=x, name=\"Deep_Neural_Collaborative_Filtering\")\n",
        "dncf.compile(optimizer=\"adam\",loss=\"binary_crossentropy\")\n",
        "\n",
        "dncf.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Deep_Neural_Collaborative_Filtering\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "user_input (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "book_input (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_embedding (Embedding)      (None, 1, 16)        1534192     user_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "book_embedding (Embedding)      (None, 1, 16)        166224      book_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "user_vec (Flatten)              (None, 16)           0           user_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "book_vec (Flatten)              (None, 16)           0           book_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Cat (Concatenate)               (None, 32)           0           user_vec[0][0]                   \n",
            "                                                                 book_vec[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_1 (Dense)          (None, 16)           528         Cat[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_2 (Dense)          (None, 16)           272         hidden_layer_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_3 (Dense)          (None, 32)           544         hidden_layer_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_4 (Dense)          (None, 32)           1056        hidden_layer_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_5 (Dense)          (None, 16)           528         hidden_layer_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "hidden_layer_6 (Dense)          (None, 16)           272         hidden_layer_5[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "sigmoid_out (Dense)             (None, 1)            17          hidden_layer_6[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,703,633\n",
            "Trainable params: 1,703,633\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63bF_QvZLhL6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ95GbLMb-Z4",
        "outputId": "c15f62da-d143-4df3-c608-e3faaf90a847"
      },
      "source": [
        "# total epochs = 8\n",
        "\n",
        "n_positive = 2048\n",
        "neg_ratio = 7\n",
        "\n",
        "gen = generate_pairs(pairs_train, n_positive, neg_ratio, pairs_test, neg_test, classification=True)\n",
        "\n",
        "hist = dncf.fit(gen, epochs=8, steps_per_epoch = len(pairs_train)//n_positive,\n",
        "    validation_data=([validation_set[:,0],validation_set[:,1]],validation_set[:,2]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "1632/1632 [==============================] - 981s 601ms/step - loss: 0.2944 - val_loss: 0.1839\n",
            "Epoch 2/8\n",
            "1632/1632 [==============================] - 993s 609ms/step - loss: 0.1727 - val_loss: 0.1159\n",
            "Epoch 3/8\n",
            "1632/1632 [==============================] - 985s 604ms/step - loss: 0.1466 - val_loss: 0.1120\n",
            "Epoch 4/8\n",
            "1632/1632 [==============================] - 974s 597ms/step - loss: 0.1323 - val_loss: 0.1240\n",
            "Epoch 5/8\n",
            "1632/1632 [==============================] - 970s 595ms/step - loss: 0.1232 - val_loss: 0.1307\n",
            "Epoch 6/8\n",
            "1632/1632 [==============================] - 965s 592ms/step - loss: 0.1166 - val_loss: 0.1486\n",
            "Epoch 7/8\n",
            "1632/1632 [==============================] - 981s 602ms/step - loss: 0.1117 - val_loss: 0.1498\n",
            "Epoch 8/8\n",
            "1632/1632 [==============================] - 982s 602ms/step - loss: 0.1080 - val_loss: 0.0956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAp15wErLmq0",
        "outputId": "eb448b7a-07f6-46cf-c430-aa8676320ba1"
      },
      "source": [
        "dncf.save(\"/content/drive/MyDrive/models/dncf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/dncf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bIzeqjJmsK"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESWuDeWECVnO"
      },
      "source": [
        "## Evaluate Top-10 Hit Rate for a given model \n",
        "\n",
        "def topKeval_test(model, k=10, n=100, times = 10000):\n",
        "  # Select one true positive at random, and 99 negative interactions\n",
        "  ct = 0\n",
        "  batch = np.zeros((n,2))\n",
        "  for j in range(times):\n",
        "    # select a user and book pair at random from test set\n",
        "    user, book = random.choice(pairs_test)\n",
        "    i=0\n",
        "    batch[i,:] = (user, book)\n",
        "    i+=1\n",
        "    while i < n:\n",
        "      # keep user the same, select book at random\n",
        "      rand_book = random.randrange(len(book_list))\n",
        "      # check if (rand_user, rand_book) is in pairs\n",
        "      if not binarySearch(pairs, (user, rand_book)):\n",
        "        batch[i,:] = (user, rand_book)\n",
        "        i+=1\n",
        "\n",
        "    # generate predictions that user has interacted with each book\n",
        "    similarity = model.predict([ batch[:,0], batch[:,1]])\n",
        "    sorted_similarities = np.argsort(similarity,axis=0)\n",
        "\n",
        "    # If 0 is in top 10, then record success\n",
        "    if 0 in sorted_similarities[-k:]:\n",
        "      ct+=1\n",
        "\n",
        "    if j%(times//5)==0 and j>0:\n",
        "      print(\"Completed iteration: \",j)\n",
        "  return 100*ct/times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oysiujasJQf1",
        "outputId": "8f2108cc-b587-4acb-a5c9-5a76f8ca41d3"
      },
      "source": [
        "topKeval_test(model = MatrixFactorization)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed iteration:  2000\n",
            "Completed iteration:  4000\n",
            "Completed iteration:  6000\n",
            "Completed iteration:  8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLS_1DoSJEys",
        "outputId": "17d26aea-7006-4aae-d4c9-f9f7fd29fded"
      },
      "source": [
        "topKeval_test(model = dncf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed iteration:  2000\n",
            "Completed iteration:  4000\n",
            "Completed iteration:  6000\n",
            "Completed iteration:  8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85.34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06w5FUoUNbRX"
      },
      "source": [
        "# Top-10 Evaluation of Popularity Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aktj8T5_PRTG",
        "outputId": "5f565e56-fc90-4c9d-d5ff-1f00efedc1e3"
      },
      "source": [
        "n=100\n",
        "k=10\n",
        "times = 10000\n",
        "ct = 0\n",
        "popularity_rank = df_interactions.book_title.value_counts()\n",
        "for j in range(times):\n",
        "    similarities = np.zeros(int(n))\n",
        "    # choose user at random\n",
        "    user = user_table[np.random.choice(user_list)]\n",
        "    # Pick a book the user has read\n",
        "    book = np.random.choice(df_interactions.book_title.loc[ df_interactions[\"user_num\"] == user ])\n",
        "    i=0\n",
        "    similarities[i] = popularity_rank[book]\n",
        "    i+=1\n",
        "    while i < n:\n",
        "        # keep user the same, select book at random\n",
        "        rand_book = random.randrange(len(book_list))\n",
        "        title = book_list[rand_book]\n",
        "        # check if (rand_user, rand_book) is in pairs\n",
        "        if not binarySearch(pairs, (user, rand_book)):\n",
        "            similarities[i] = popularity_rank[title]\n",
        "            i+=1\n",
        "    sorted_similarities = np.argsort(similarities,axis=0)\n",
        "\n",
        "    if 0 in sorted_similarities[-10:]:\n",
        "        ct+=1\n",
        "    if j%(times//5)==0 and j>0:\n",
        "        print(\"Completed iteration: \",j)\n",
        "print(100*ct/times)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed iteration:  2000\n",
            "Completed iteration:  4000\n",
            "Completed iteration:  6000\n",
            "Completed iteration:  8000\n",
            "0.6217\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}